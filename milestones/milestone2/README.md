### **Milestone 2: Understanding RAG & Core Technologies**
**Goal:** Understand Retrieval-Augmented Generation (RAG) architecture and build a working RAG system with local LLM.

#### Objectives:
- Study RAG architecture (retriever, generator, integration).
- Install and configure a local LLM (Ollama with Qwen2.5:7b).
- Write scripts to interact with the chosen LLM and index documents.
- Apply Google Python Style Guide (type hints, docstrings, naming conventions).

#### Deliverables:
- [x] Discussion summary of RAG concepts
- [x] LocalLLM class for Ollama integration
  - `models/llm_model.py` - LLM wrapper for text generation
- [x] DocumentRetriever class for indexing and retrieval
  - `models/retriever.py` - Document indexing with LlamaIndex
- [x] MVC Architecture Implementation
  - `models/` - LLM and Retriever models
  - `views/view.py` - Display/presentation layer
  - `controllers/rag_controller.py` - Coordinates RAG workflow
  - `main.py` - Main entry point for RAG system
- [x] Sample data for testing
  - `data/sample.txt` - Sample document for indexing

---

# ðŸ“˜ Discussion Summary â€” RAG Concepts

## **What is RAG (Retrieval-Augmented Generation)?**

RAG is an AI framework that combines **information retrieval** with **text generation** to produce more accurate and contextually relevant responses.

### **Key Components:**

#### **1. Retriever**
- Searches through a knowledge base (documents, databases)
- Uses embeddings and vector similarity to find relevant information
- Returns top-k most relevant documents based on query

#### **2. Generator (LLM)**
- Takes retrieved documents as context
- Generates responses based on the retrieved information
- Produces more accurate answers grounded in actual data

#### **3. Integration**
- Query â†’ Retriever finds relevant docs â†’ LLM generates answer using docs
- Reduces hallucinations by grounding responses in real data
- Allows LLMs to access up-to-date or domain-specific information

---

## **Why Use RAG?**

âœ… **Reduces hallucinations** - Answers based on actual documents  
âœ… **Up-to-date information** - Can index recent documents  
âœ… **Domain-specific knowledge** - Add your own data  
âœ… **Cost-effective** - No need to retrain LLMs  
âœ… **Transparent** - Can cite sources used

---

## **Technologies Used in This Implementation**

### **LLM: Ollama + Qwen2.5:7b**
- **Ollama:** Tool for running LLMs locally
- **Qwen2.5:7b:** 7-billion parameter model by Alibaba
- Runs completely offline, no API costs

### **Indexing: LlamaIndex**
- Framework for connecting LLMs with external data
- Handles document loading, chunking, and indexing
- Provides query engines for retrieval

### **Embeddings: HuggingFace (BAAI/bge-small-en-v1.5)**
- Converts text to vector representations
- Free and runs locally
- Efficient for similarity search

### **Vector Storage**
- In-memory vector store (no external database needed)
- Fast for small-to-medium datasets
- Can be upgraded to FAISS, Chroma, or Weaviate later

---

## ðŸ“‚ Project Structure

```
milestone2/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ main.py                            # Main entry point
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ llm_model.py                   # LocalLLM wrapper for Ollama
â”‚   â””â”€â”€ retriever.py                   # DocumentRetriever with LlamaIndex
â”œâ”€â”€ views/
â”‚   â””â”€â”€ view.py                        # Display layer
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ rag_controller.py              # RAG workflow coordinator
â””â”€â”€ data/
    â””â”€â”€ sample.txt                     # Sample document
```

---

## ðŸš€ Getting Started

### Prerequisites
- Python 3.10 or higher
- Ollama installed ([ollama.ai](https://ollama.ai))
- Git

### Installation

1. **Install Ollama and pull the model**
   ```bash
   # Install Ollama from https://ollama.ai
   
   # Pull the Qwen model
   ollama pull qwen2.5:7b
   
   # Verify Ollama is running
   ollama list
   ```

2. **Navigate to milestone2 folder**
   ```bash
   cd milestone2
   ```

3. **Create virtual environment** (Optional but recommended)
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On macOS/Linux
   ```

4. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## ðŸŽ® Running the Project

### **Run the Full RAG System:**
```bash
python3 main.py
```

This will:
1. Load documents from `data/sample.txt`
2. Build vector index
3. Start interactive Q&A session
4. Type 'quit' to exit

### **Test Individual Components:**

**Test LLM only:**
```bash
python3 models/llm_model.py
```

**Test Retriever only:**
```bash
python3 models/retriever.py
```

---

## ðŸ’¡ Example Usage

```
Building document index...
Indexed 1 documents.

Enter your question (or 'quit' to exit): What is Python?

Retrieving relevant documents...
Generating response...
Answer: Python is a high-level, interpreted programming language 
known for its simplicity and readability...
```

---

## ðŸ“š Key Learnings

### **RAG Architecture**
- âœ… Understanding retriever-generator pipeline
- âœ… Embedding-based similarity search
- âœ… Context-aware response generation

### **Local LLM Integration**
- âœ… Running Ollama locally
- âœ… Connecting Python to LLM
- âœ… Error handling and timeouts

### **Document Indexing**
- âœ… Converting text to embeddings
- âœ… Vector storage and retrieval
- âœ… Query processing

### **MVC Architecture**
- âœ… Separating concerns (Model, View, Controller)
- âœ… Modular and testable code
- âœ… Clean code principles

